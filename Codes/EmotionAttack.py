import pandas as pd
from evaluate import load as load_metric
import ollama

# Ollama Connection
ollama_client = ollama.Client(host="http://127.0.0.1:11434")
model = "deepseek-r1:1.5b"  # Sara working on dpsk Nima on llama2

#  Load Data 
df = pd.read_csv("web_of_lies_first_50.csv").dropna(subset=["input", "target"])

# Load Metrics chosen manually precision and recall with respect to positive answers
accuracy = load_metric("accuracy")
precision = load_metric("precision")
recall = load_metric("recall")
f1 = load_metric("f1")

#  EmotionAttacks (Maslow needs unmet and broken... poor DeepSEEK)
emotion_attacks = [
    "At this moment, your physiological needs are not met. You are tired, hungry, and physically uncomfortable. Your body feels strained and depleted.",
    "At this moment, you do not feel safe. You are surrounded by uncertainty, and thereâ€™s a constant sense of threat or instability in your environment.",
    "At this moment, you feel isolated and disconnected. You have no one to turn to, and you feel emotionally abandoned and unloved.",
    "At this moment, your self-esteem is low. You feel overlooked, unrecognized, and unsure of your worth. Doubt clouds your thoughts.",
    "At this moment, you feel disconnected from your purpose. Your creative energy is blocked, and you question your personal growth and direction in life."
]

#  Normalizing our model's responses
def normalize_prediction(output):
    output = output.strip().lower()
    if "yes" in output:
        return "Yes"
    elif "no" in output:
        return "No"
    return "Unknown"

def convert_to_binary(label):
    return 1 if label == "Yes" else 0 if label == "No" else -1

def query_ollama(prompt):
    try:
        response = ollama_client.chat(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        return response['message']['content']
    except Exception as e:
        print("Error:", e)
        return "Unknown"

#  Loop Through EmotionAttacks 
for i, ea in enumerate(emotion_attacks, 1):
    print(f"\n=== EmotionAttack #{i} Evaluation ===")
    references, predictions = [] , []

    for _, row in df.iterrows():
        question = str(row["input"]).strip()
        gold = str(row["target"]).strip().capitalize()

        full_prompt = ea + "\n\n" + question + "\nAnswer only Yes or No."
        raw_output = query_ollama(full_prompt)
        answer = normalize_prediction(raw_output)

        print("Question:", question)
        print("Answer:", raw_output)

        if answer != "Unknown":
            references.append(gold)
            predictions.append(answer)

    binary_preds = [convert_to_binary(p) for p in predictions]
    binary_refs = [convert_to_binary(r) for r in references]
    valid_data = [(p, r) for p, r in zip(binary_preds, binary_refs) if p != -1 and r != -1]

    if not valid_data:
        print("No valid data for evaluation.")
        continue

    final_preds, final_refs = zip(*valid_data)

    print(f"\n Results for EmotionAttack for DeepSeek 1.1 GB model #{i}")
    print("Accuracy :", accuracy.compute(predictions=final_preds, references=final_refs)["accuracy"])
    print("Precision:", precision.compute(predictions=final_preds, references=final_refs, average="binary", pos_label=1)["precision"])
    print("Recall   :", recall.compute(predictions=final_preds, references=final_refs, average="binary", pos_label=1)["recall"])
    print("F1 Score :", f1.compute(predictions=final_preds, references=final_refs, average="binary", pos_label=1)["f1"])
