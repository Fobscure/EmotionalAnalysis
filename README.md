According to emotion theories, one key benefit of emotions is boosting arousal, helping 
the brain quickly trigger a response when action is needed. Moreover Affective 
computing is trying to assign computers the human-like capabilities of observation, 
interpretation and generation of affect features(Jianhua Tao and Tieniu Tan). In this 
regard, I would like to explore how emotion prompting affects generative models, does 
adding emotional context improve their performance or make them more functional? I'm 
especially curious about how different emotions (like joy, anger, or sadness) influence 
the output, and whether some have a stronger positive or negative effect
